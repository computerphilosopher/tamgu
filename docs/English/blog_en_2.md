# TAMGU

Today, thanks to Machine Learning algorithms, speech recognition or machine translation has entered the lives of millions of people. Unfortunately, for these algorithms to work optimally, they need annotated data, a lot of annotated and structured data.
This problem is at the heart of most Artificial Intelligence teams around the world. How to have enough data to train efficient and reliable machine learning models?

One of the solutions is to generate these corpora automatically using specialized tools. Christopher Ré (PR at Stanford) has given this approach a name: Data Programming. We now oppose corpora annotated by hand by human beings to corpora created semi-automatically by artificial means: Gold Standards vs Silver Standards. 
While Christopher Ré's group has produced its own set of tools (Snorkel), we have decided to address this problem from a very different angle. 

## Formal grammars

My main research focus since the beginning of my studies has been in the field of formal grammars. For many years, I have participated in the construction of a rules engine that has been widely used in evaluation campaigns in European and French projects. But, since the world of research is what it is, these so-called symbolic methods have gradually lost their importance in favour of statistical approaches. That is until the advent of Data Programming. By a strange paradox, if the linguistic rules of the past have lost their lustre, Data Programming has given them a new lease of life. Because detecting word patterns means applying linguistic rules, even if they are much poorer than those we used to manipulate 10 years ago. My experience in the implementation of rule engines has found there a new study space. On the other hand, this time, instead of a specialized tool, I decided to create a new programming language.

## Tamgu

Tamgu was born from these reflections. It was also born from my discussions with Machine Learning people to identify the most common problems and how to solve them. In a few words, in a very simplified way, Machine Learning needs annotated corpora in which recurrent patterns of words are identified and labelled. They also need synthetic corpora in which textual data are generated by following specific grammars. In particular, in machine translation, it can be very interesting to have noisy corpora, corpora where known errors are artificially introduced so that the model can learn to recognize them and translate them correctly.

#### FIL

Thus, such a language must provide both something to annotate a corpus, but also something to generate new texts, while offering the widest possible range of instructions of all kinds to avoid restricting the user in his work.

Tamgu is a modern language that belongs to a particular class of programming languages: FIL languages: Functional, Imperative and Logical.

The majority of languages today are IF, Imperative with a touch of Functionality as Kotlin or Swift can be. Others are frankly functional like Haskell or Lisp. Finally, some are purely logical like Prolog.

However, the choice of an FIL language poses particular interoperability problems. If imperative and functional paradigmes can easily communicate, the addition of a logic module poses particular problems. However, if we examine the objects manipulated by all these approaches, we can easily discover a common point: they more or less manipulate the same objects: numbers, strings and containers. Thus, I was freely inspired by Haskell for the functional part, which proved to be the ideal language to write compact and efficient lambdas functions. I adapted it by allowing the language to manipulate Tamgu dictionaries (maps) directly or to iterate on external variables.

```C++

//we create the vector:[1,2,3,3,4,5,6,7,8,9,10] 
vector v =[1...10];

//The'<...>' identified to Haskell expression
vector vv = <map (+1) v>; //we add one to each element

```
Prolog has long been the language of choice for building text generators. The syntax of the language allows rich and complex grammars to be written in a few lines. However, Prolog has some limitations like the difficulty of managing large lexicons, which can be solved by integrating them into an FIL environment by giving this management to suitable objects. For the logical part, I also had to slightly modify the syntax of the language to make it interoperable with the rest of Tamgu. For example, variables are identified in Prolog by a capital letter at the beginning of their name, while lowercase words are considered immutable atoms, which is problematic in an FIL language that has none of these restrictions. I was inspired by the SPARQL syntax where variables in logical expressions start with a"? ». Thus, by identifying the variables differently, I was able to replace the atoms with character strings. As for the Tamgu vectors, they are transparently reinterpreted as Prolog vectors. More precisely, Tamgu vectors now have the ability to be unified in a Prolog execution. The default unification is reduced to a simple comparison of equality between two objects, which then allows any Tamgu object to be used in a logical expression. 

```PROLOG

concat([],?X,?X,?X).
concat([?H|?T],?Y, [?H|?Z]) :- concat(?T,?Y,?Z).

//You can mix freely your predicate definition with some regular tamgu code
v=concat(['english','russian','french'],['spanish'],?L);
println(v); //['english','russian','french','spanish']

```

## Language Primitives

The "imperative" part of the language is composed of the traditional modules of most existing languages. We can declare variables, functions, threads (micro-threads even), and classes. 

```C++
function callme(int i, string s) {...}

thread mythread(int i, string u) {...}

frame test {...}
```

Unlike Python, the language requires that variables be declared with a type. I also tried to simplify the handling of threads by implicitly protecting all potentially dangerous variables (mainly containers). Tamgu offers a very rich range of different types: strings, floats, integers, vectors, maps. 

```C++

int i = 10;
float f = 1,234;

string s = "The dog is <eating> a bone.";

vector v =[1,2,3];

map m = {'a':1,'b':2};

``` 

### Character string management

In particular, Tamgu provides an impressive arsenal for manipulating your strings. First of all, it dynamically recognizes the encoding of a string, even if it accidentally mixes different encodings.
There are many ways to access the content of a string in Tamgu.

* With indexes: str[1], str[2 :4], str[-2 :]
* With sub-strings: str["beg" : "end"]
* With regular expressions: str[r "ab%d+"].

You can also chain the descriptions: str["a":" e"][1:-1]
But most importantly, you can modify the content of a string in this way:

```C++
string s="The dog is <eating> a bone.";

s["<":">"][1:-1] = "biting"; //the string is now: The dog is <biting> a bone.

The first part isolates the substring between' <...>', 
then removes them to focus on the content of
this substring and replaces it with a new value.
```
## Glossaries and Rules

Last but not least, Tamgu offers a lexicon mechanism based on transducers. Transducers are the most efficient solution for encoding a lexicon. They offer both compactness and speed of access. The version I implemented also allows you to identify a word by traversing the transducer with an edit distance. In this way, it is possible to recognize words with common errors such as switching two characters, missing a character or, on the contrary, the presence of a supernumerary character. 

But above all, these lexicons can be coupled with context-free rules, which can be _written directly in the code_. It is therefore possible to write your own vocabulary, add general lexicons of English or French if necessary and then write rules to identify complex patterns. In the following example, we define a few words to which we associate the label _food_. Then we create a simple rule that detects the sequence _the food_. We create an _annotator_ that will automatically be associated with these rules and we apply it to our sentence.


```C++
//We define some lexical rules (starting with a "@")
@food <- burger.
@food <- tartare.


//Our rule: if a "food" word is found in a sentence, then we return a "meal" label 
meal <- "the", #food.

//We need a specific object to scan a sentence
annotator r;

//a sentence
string sentence="Here, the burger and the tartare are delicious."
vector v = r.parse(sentence); 

//Result: v =  [['meal',[10,16]],['meal',[25,32]]]
//It reads: two 'meal' were found at position 10-16 and position 25-32...
```

Thus, in a few lines, we can describe a lexicon coupled with rules that allow us to detect in the text the positions of the textual elements that interest us.

The example is very simple, but you can increase the vocabulary at your leisure, it is compiled as a transducer on the fly, and also the number of rules. Again, there is full interoperability between this mechanism and the FIL language.

## Libraries

To conclude, Tamgu is not a fixed language. Its architecture has the particularity that the implementation of an external library obeys exactly the same rules, the same derivation more precisely, as an internal object. In other words, implementing an embedding mechanism based on Word2Vec corresponds more or less to the implementation of the _string type. Unlike Java or Python, which only allow the implementation of external methods, there is a direct correspondence between a Tamgu object and its implementation as a C++ object. 
Tamgu offers libraries that encapsulate cURL, liblinear, word2vec, SQLite, FLTK (GUI) or Wapiti (CRF). You can easily add yours as Tamgu also provides a very simple script to produce your own template to create your own library.
## Conclusion

Tamgu has been designed to make cleaning or corpus creation tasks as simple as possible. You can now identify complex patterns in a few lines of code or generate text by applying grammars. The language is very easy to learn, especially because it does not diverge from known programming standards. Yet it offers all the power needed to give your Machine Learning algorithms the data they need.

